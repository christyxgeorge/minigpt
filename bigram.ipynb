{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/s_char/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of text = {len(text)}\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ", ,!,$,&,',,,-,.,3,:,;,?,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z\n",
      "Vocab Size = 65\n"
     ]
    }
   ],
   "source": [
    "# Unique Characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(','.join(chars))\n",
    "print(f\"Vocab Size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch: i for i, ch in enumerate(chars)}\n",
    "itos = { i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [ stoi[c] for c in s] # encode a string to a list of integers\n",
    "decode = lambda l: ''.join([ itos[i] for i in l]) # decode a list of integers back to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello world\"))\n",
    "print(decode(encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary = 50257\n",
      "31373: b'hello'\n",
      "1312: b' i'\n",
      "716: b' am'\n",
      "6769: b' wave'\n",
      "27794: b'boarding'\n",
      "3548: b'??'\n"
     ]
    }
   ],
   "source": [
    "# Sub-word tokenizer --> used by GPT2,3...\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f\"Vocabulary = {enc.n_vocab}\") # len(enc._mergeable_ranks) + len(enc._special_tokens)\n",
    "tokens = enc.encode('hello i am waveboarding??')\n",
    "all_tokens = { **enc._mergeable_ranks, **enc._special_tokens }\n",
    "token_mapping = { tkn_id: tkn for tkn, tkn_id in all_tokens.items() }\n",
    "for tkn_id in tokens: print(f\"{tkn_id}: {token_mapping.get(tkn_id, 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1115394]), Dtype: torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire text and store into torch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Shape: {data.shape}, Dtype: {data.dtype}\")\n",
    "print(data[:100]) ## Looks like 0 = \\n, 1 = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Index = 1003854, Training len = 1003854, Validation len = 111540\n"
     ]
    }
   ],
   "source": [
    "# Split in train, val\n",
    "tv_split = int(0.9*len(data))\n",
    "train_data = data[:tv_split]\n",
    "val_data = data[tv_split:]\n",
    "print(f\"Split Index = {tv_split}, Training len = {len(train_data)}, Validation len = {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8  ### Context Window [Max. Context length for predictions]\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context = tensor([18]) => Target = 47\n",
      "Context = tensor([18, 47]) => Target = 56\n",
      "Context = tensor([18, 47, 56]) => Target = 57\n",
      "Context = tensor([18, 47, 56, 57]) => Target = 58\n",
      "Context = tensor([18, 47, 56, 57, 58]) => Target = 1\n",
      "Context = tensor([18, 47, 56, 57, 58,  1]) => Target = 15\n",
      "Context = tensor([18, 47, 56, 57, 58,  1, 15]) => Target = 47\n",
      "Context = tensor([18, 47, 56, 57, 58,  1, 15, 47]) => Target = 58\n"
     ]
    }
   ],
   "source": [
    "# Get the examples from `block_size+1` bytes of data\n",
    "# x = train_data[:block_size]  ### Input\n",
    "# y = train_data[1:block_size+1] ### Output labels for the above inputs\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print(f\"Context = {context} => Target = {target}\")\n",
    "\n",
    "# Do the same with one array!\n",
    "chunk = train_data[:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = chunk[:t+1]\n",
    "    target = chunk[t+1]\n",
    "    print(f\"Context = {context} => Target = {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size): ## Batch Size = Number of sequences being processed in parallel!\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Generate `batch_size` random offsets \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix ]) # Each sample is stacked as a row!\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix ])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: xb = torch.Size([4, 8]), yb = torch.Size([4, 8])\n",
      "Batch 0, Block/Time 0 => Context = tensor([24]) => Target = 43\n",
      "Batch 0, Block/Time 1 => Context = tensor([24, 43]) => Target = 58\n",
      "Batch 0, Block/Time 2 => Context = tensor([24, 43, 58]) => Target = 5\n",
      "Batch 0, Block/Time 3 => Context = tensor([24, 43, 58,  5]) => Target = 57\n",
      "Batch 0, Block/Time 4 => Context = tensor([24, 43, 58,  5, 57]) => Target = 1\n",
      "Batch 0, Block/Time 5 => Context = tensor([24, 43, 58,  5, 57,  1]) => Target = 46\n",
      "Batch 0, Block/Time 6 => Context = tensor([24, 43, 58,  5, 57,  1, 46]) => Target = 43\n",
      "Batch 0, Block/Time 7 => Context = tensor([24, 43, 58,  5, 57,  1, 46, 43]) => Target = 39\n",
      "Batch 1, Block/Time 0 => Context = tensor([44]) => Target = 53\n",
      "Batch 1, Block/Time 1 => Context = tensor([44, 53]) => Target = 56\n",
      "Batch 1, Block/Time 2 => Context = tensor([44, 53, 56]) => Target = 1\n",
      "Batch 1, Block/Time 3 => Context = tensor([44, 53, 56,  1]) => Target = 58\n",
      "Batch 1, Block/Time 4 => Context = tensor([44, 53, 56,  1, 58]) => Target = 46\n",
      "Batch 1, Block/Time 5 => Context = tensor([44, 53, 56,  1, 58, 46]) => Target = 39\n",
      "Batch 1, Block/Time 6 => Context = tensor([44, 53, 56,  1, 58, 46, 39]) => Target = 58\n",
      "Batch 1, Block/Time 7 => Context = tensor([44, 53, 56,  1, 58, 46, 39, 58]) => Target = 1\n",
      "Batch 2, Block/Time 0 => Context = tensor([52]) => Target = 58\n",
      "Batch 2, Block/Time 1 => Context = tensor([52, 58]) => Target = 1\n",
      "Batch 2, Block/Time 2 => Context = tensor([52, 58,  1]) => Target = 58\n",
      "Batch 2, Block/Time 3 => Context = tensor([52, 58,  1, 58]) => Target = 46\n",
      "Batch 2, Block/Time 4 => Context = tensor([52, 58,  1, 58, 46]) => Target = 39\n",
      "Batch 2, Block/Time 5 => Context = tensor([52, 58,  1, 58, 46, 39]) => Target = 58\n",
      "Batch 2, Block/Time 6 => Context = tensor([52, 58,  1, 58, 46, 39, 58]) => Target = 1\n",
      "Batch 2, Block/Time 7 => Context = tensor([52, 58,  1, 58, 46, 39, 58,  1]) => Target = 46\n",
      "Batch 3, Block/Time 0 => Context = tensor([25]) => Target = 17\n",
      "Batch 3, Block/Time 1 => Context = tensor([25, 17]) => Target = 27\n",
      "Batch 3, Block/Time 2 => Context = tensor([25, 17, 27]) => Target = 10\n",
      "Batch 3, Block/Time 3 => Context = tensor([25, 17, 27, 10]) => Target = 0\n",
      "Batch 3, Block/Time 4 => Context = tensor([25, 17, 27, 10,  0]) => Target = 21\n",
      "Batch 3, Block/Time 5 => Context = tensor([25, 17, 27, 10,  0, 21]) => Target = 1\n",
      "Batch 3, Block/Time 6 => Context = tensor([25, 17, 27, 10,  0, 21,  1]) => Target = 54\n",
      "Batch 3, Block/Time 7 => Context = tensor([25, 17, 27, 10,  0, 21,  1, 54]) => Target = 39\n"
     ]
    }
   ],
   "source": [
    "# Adding the batch dimension to the examples\n",
    "torch.manual_seed(1337) ## To get repeatability!\n",
    "\n",
    "def print_batch(batch_size):\n",
    "    for b in range(batch_size):\n",
    "        for t in range(block_size):\n",
    "            context = xb[b, :t+1]\n",
    "            target = yb[b, t]\n",
    "            print(f\"Batch {b}, Block/Time {t} => Context = {context} => Target = {target}\")\n",
    "\n",
    "xb, yb = get_batch('train', batch_size=4)\n",
    "print(f\"Shapes: xb = {xb.shape}, yb = {yb.shape}\")\n",
    "print_batch(batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape = torch.Size([32, 65]), Loss = 4.705134868621826, Expected loss = ln(1/n) = 4.174387454986572\n",
      "\n",
      "P-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3!dcb\n"
     ]
    }
   ],
   "source": [
    "# Bigram language model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token gets the logits for the next token from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx, targets --> B x T (batch_size x block_size)\n",
    "        logits = self.token_embedding_table(idx) # B x T x C [vocab_size]\n",
    "        # loss = F.cross_entropy(logits, targets) # Does not work because pytorch needs B * C * T for multi-dimensional array\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # So, Reshaping so that cross_entropy works\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_tokens): ## Generate next `num_tokens` tokens, idx --> B x T\n",
    "        for _ in range(num_tokens):\n",
    "            logits, loss = self(idx) ## logits --> B x T x C\n",
    "            # focus on last time step (only the last character) \n",
    "            logits = logits[:, -1, :] ## --> B x C\n",
    "            # counts = logits.exp() # counts, equivalent to N\n",
    "            # probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "            probs = F.softmax(logits, dim=-1) ## --> B x C\n",
    "            # Sample from the probability distribution to get the next idx.\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) ## --> B x 1\n",
    "            idx = torch.cat((idx, idx_next), dim=1) ## --> B x T+1\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "xb, yb = get_batch('train', batch_size=4)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f\"Logits Shape = {logits.shape}, Loss = {loss}, Expected loss = ln(1/n) = {-1 * torch.log(torch.tensor(1/vocab_size))}\")\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long) ## Create the initial 'text' to generate the continuation --> Using 0 = \\n\n",
    "tokens = m.generate(idx, num_tokens=100)\n",
    "print(decode(tokens[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an optimizer object\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # instead of torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss = 2.436946392059326\n",
      "Step 1000, Loss = 2.48052716255188\n",
      "Step 2000, Loss = 2.426077127456665\n",
      "Step 3000, Loss = 2.4889259338378906\n",
      "Step 4000, Loss = 2.3345420360565186\n",
      "Step 5000, Loss = 2.4978251457214355\n",
      "Step 6000, Loss = 2.440619945526123\n",
      "Step 7000, Loss = 2.3754453659057617\n",
      "Step 8000, Loss = 2.4478201866149902\n",
      "Step 9000, Loss = 2.5060737133026123\n",
      "2.5448358058929443\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # Increase batch size to 32\n",
    "for step in range(10000): ## `n` steps\n",
    "    xb, yb = get_batch('train', batch_size=batch_size) ## xb = B x T\n",
    "    # print(f\"Shapes: {xb.shape} / {yb.shape}\")\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) ## Zero out existing gradients computed for previous step\n",
    "    loss.backward()\n",
    "    optimizer.step() ## change the weights based on the gradients\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Loss = {loss.item()}\")\n",
    "    # print(loss.item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARCKICOMave wap\n",
      "\n",
      "I RO:\n",
      "Banleenoalit-blt\n",
      "INRon\n",
      "\n",
      "UM: nd kngonesll;\n",
      "O: pa heore 'ga llis?-sur inidind;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regenerate after training for 100+1000+10000 cycles/steps\n",
    "idx = torch.zeros((1,1), dtype=torch.long) ## Create the initial 'text' to generate the continuation --> Using 0 = \\n\n",
    "tokens = m.generate(idx, num_tokens=100)\n",
    "print(decode(tokens[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([4, 8]), torch.Size([4, 8])\n",
      "Logits Shape = torch.Size([4, 8, 65])\n",
      "Cross Entropy Loss = 2.570775270462036\n",
      "Log Probs Shape = torch.Size([4, 8, 65]), All Close = True -- Targets Shape = torch.Size([4, 8])\n",
      "Computed Loss = 2.570775032043457, True\n"
     ]
    }
   ],
   "source": [
    "# Basically, checking how cross_entropy is implemented under the hood~\n",
    "i = 0\n",
    "xb = torch.stack([data[i:i+block_size] for i in range(0,4*block_size,block_size) ]) # Each sample is stacked as a row!\n",
    "yb = torch.stack([data[i+1:i+block_size+1] for i in range(0,4*block_size,block_size) ])\n",
    "print(f\"Shapes: {xb.shape}, {yb.shape}\")\n",
    "logits = m.token_embedding_table(xb)\n",
    "print(f\"Logits Shape = {logits.shape}\")\n",
    "B,T,C = logits.shape\n",
    "loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T)) # logits[0], yb[0]) # Softmax followed by NLL Loss\n",
    "print(f\"Cross Entropy Loss = {loss.item()}\")\n",
    "\n",
    "# Computing the Softmax in the next two lines\n",
    "logits_max = logits - logits.max(2, keepdims=True).values ## for numerical stability, probs dont change\n",
    "# print(f\"Logits Max = {logits_max[0][0]}\")\n",
    "# print(f\"Logits = {logits[0][0]}\")\n",
    "counts = logits_max.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(2, keepdims=True) # probabilities for next character / Softmax\n",
    "logprobs = torch.log(probs)\n",
    "log_softmax = F.log_softmax(logits, dim=2)\n",
    "print(f\"Log Probs Shape = {logprobs.shape}, All Close = {torch.allclose(logprobs, log_softmax)} -- Targets Shape = {yb.shape}\")\n",
    "\n",
    "xloss = -1 * torch.sum(logprobs * F.one_hot(yb)) / (B * T)\n",
    "print(f\"Computed Loss = {xloss.item()}, {torch.isclose(loss, xloss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    logits: torch.Tensor, targets: torch.Tensor | None = None) -> torch.Tensor | None:\n",
    "    \"\"\"Compute loss for the predicted logits v/s the targets\"\"\"\n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        # loss = F.cross_entropy(logits, targets) # Does not work because pytorch needs B * C * T for multi-dimensional array\n",
    "        # So, Reshaping so that cross_entropy works\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "    return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
